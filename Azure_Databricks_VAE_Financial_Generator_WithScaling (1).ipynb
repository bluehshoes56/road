{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Synthetic Financial Data Generator (Real Data Version)\n",
    "\n",
    "**Enterprise-grade synthetic data generation for your actual financial transactions**\n",
    "\n",
    "- **Privacy-preserving**: GDPR, PCI-DSS, GLBA compliance\n",
    "- **Scalable**: 3.5K → 552K+ rows with configurable parameters\n",
    "- **Advanced scaling**: Optional million+ row generation\n",
    "- **High-fidelity**: 90%+ statistical similarity preservation\n",
    "- **Business logic**: Domain-specific constraints and correlations\n",
    "- **Edge case generation**: Synthetic rare events not in original data\n",
    "\n",
    "**Optimized for Azure Databricks**: GPU acceleration, distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Package Installation and Setup\n",
    "# Run this cell first to install required packages\n",
    "\n",
    "# Install packages compatible with Databricks runtime\n",
    "%pip install tensorflow==2.13.0 --quiet\n",
    "%pip install plotly kaleido --quiet\n",
    "%pip install numpy==1.24.3 --quiet\n",
    "\n",
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU devices available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Setup complete - Ready for VAE training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Configuration Settings\n",
    "# Modify these parameters to scale your deployment\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Global configuration for VAE synthetic data generation.\"\"\"\n",
    "    \n",
    "    # ===========================================\n",
    "    # DATASET SCALING (Change this to scale up)\n",
    "    # ===========================================\n",
    "    DATASET_SIZES = {\n",
    "        'PROTOTYPE': 3500,      # 5-10 minutes\n",
    "        'SMALL': 25000,         # 30-45 minutes\n",
    "        'MEDIUM': 100000,       # 1-2 hours\n",
    "        'LARGE': 250000,        # 2-3 hours\n",
    "        'FULL': 552000          # 3-4 hours\n",
    "    }\n",
    "    \n",
    "    CURRENT_SIZE: str = 'PROTOTYPE'  # ← Change this to scale up\n",
    "    \n",
    "    # ===========================================\n",
    "    # ADVANCED SCALING (Future Use - Keep Disabled for Immediate Work)\n",
    "    # ===========================================\n",
    "    # Toggle these parameters when you need million+ row generation\n",
    "    ENABLE_ADVANCED_SCALING: bool = False      # Set True for million+ rows\n",
    "    CUSTOM_OUTPUT_SIZE: Optional[int] = None   # e.g., 5000000 for 5M rows\n",
    "    BATCH_GENERATION: bool = False             # Set True for memory efficiency\n",
    "    GENERATION_BATCH_SIZE: int = 100000        # Rows per batch\n",
    "    \n",
    "    # ===========================================\n",
    "    # VAE ARCHITECTURE\n",
    "    # ===========================================\n",
    "    LATENT_DIM: int = 16           # Recommended: 8-32 for financial data\n",
    "    ENCODER_LAYERS: List[int] = field(default_factory=lambda: [256, 128, 64])     # Encoder hidden layers\n",
    "    DECODER_LAYERS: List[int] = field(default_factory=lambda: [64, 128, 256])     # Decoder hidden layers\n",
    "    ACTIVATION: str = 'relu'                       # Activation function\n",
    "    \n",
    "    # ===========================================\n",
    "    # TRAINING PARAMETERS\n",
    "    # ===========================================\n",
    "    BATCH_SIZE: int = 256          # Optimized for GPU memory\n",
    "    EPOCHS: int = 100              # Training epochs (5 min target for prototype)\n",
    "    LEARNING_RATE: float = 1e-3    # Adam optimizer learning rate\n",
    "    BETA_KL: float = 1.0           # KL divergence weight (β-VAE parameter)\n",
    "    \n",
    "    # ===========================================\n",
    "    # DATA SCHEMA (Based on your uploaded data)\n",
    "    # ===========================================\n",
    "    CATEGORICAL_COLUMNS: List[str] = field(default_factory=lambda: [\n",
    "        'payer_Company_Name', 'payee_Company_Name', \n",
    "        'payer_industry', 'payee_industry',\n",
    "        'payer_GICS', 'payee_GICS',\n",
    "        'payer_subindustry', 'payee_subindustry'\n",
    "    ])\n",
    "    \n",
    "    NUMERICAL_COLUMNS: List[str] = field(default_factory=lambda: [\n",
    "        'ed_amount', 'fh_file_creation_date', 'fh_file_creation_time'\n",
    "    ])\n",
    "    \n",
    "    # ===========================================\n",
    "    # QUALITY TARGETS\n",
    "    # ===========================================\n",
    "    STATISTICAL_MATCH_RATIO: float = 0.90  # 90% statistical similarity\n",
    "    EDGE_CASE_RATIO: float = 0.10          # 10% edge case generation\n",
    "    MIN_CORRELATION_PRESERVATION: float = 0.85\n",
    "    \n",
    "    # ===========================================\n",
    "    # PRIVACY & COMPLIANCE\n",
    "    # ===========================================\n",
    "    PRIVACY_MODE: str = 'STANDARD'  # Options: STANDARD, GDPR, PCI_DSS, GLBA\n",
    "    ENABLE_DIFFERENTIAL_PRIVACY: bool = False\n",
    "    DP_EPSILON: float = 2.0\n",
    "    DP_DELTA: float = 1e-5\n",
    "    \n",
    "    # Privacy settings by compliance mode\n",
    "    PRIVACY_SETTINGS: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {\n",
    "        'STANDARD': {'min_k_anonymity': 5, 'enable_noise_injection': False},\n",
    "        'GDPR': {'min_k_anonymity': 10, 'enable_noise_injection': True},\n",
    "        'PCI_DSS': {'min_k_anonymity': 15, 'enable_noise_injection': True},\n",
    "        'GLBA': {'min_k_anonymity': 8, 'enable_noise_injection': True}\n",
    "    })\n",
    "    \n",
    "    # ===========================================\n",
    "    # BUSINESS CONSTRAINTS\n",
    "    # ===========================================\n",
    "    COLUMN_RANGES: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {\n",
    "        'ed_amount': {'min': 0.01, 'max': 1000000.0},\n",
    "        'fh_file_creation_date': {'min': 250000, 'max': 260000},\n",
    "        'fh_file_creation_time': {'min': 0, 'max': 2359}\n",
    "    })\n",
    "    \n",
    "    # ===========================================\n",
    "    # AZURE DATABRICKS OPTIMIZATION\n",
    "    # ===========================================\n",
    "    USE_GPU_ACCELERATION: bool = True\n",
    "    ENABLE_DISTRIBUTED_TRAINING: bool = True\n",
    "    MEMORY_OPTIMIZATION: bool = True\n",
    "    \n",
    "    def get_current_dataset_size(self) -> int:\n",
    "        return self.DATASET_SIZES[self.CURRENT_SIZE]\n",
    "    \n",
    "    def get_synthetic_output_size(self) -> int:\n",
    "        \"\"\"Get target synthetic data output size.\"\"\"\n",
    "        if self.ENABLE_ADVANCED_SCALING and self.CUSTOM_OUTPUT_SIZE:\n",
    "            return self.CUSTOM_OUTPUT_SIZE\n",
    "        return self.get_current_dataset_size()\n",
    "    \n",
    "    def get_privacy_settings(self) -> Dict:\n",
    "        return self.PRIVACY_SETTINGS[self.PRIVACY_MODE]\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Dataset Size: {config.CURRENT_SIZE} ({config.get_current_dataset_size():,} rows)\")\n",
    "print(f\"Advanced Scaling: {'ENABLED' if config.ENABLE_ADVANCED_SCALING else 'DISABLED (Standard Mode)'}\")\n",
    "if config.ENABLE_ADVANCED_SCALING:\n",
    "    print(f\"Custom Output Size: {config.get_synthetic_output_size():,} rows\")\n",
    "    scaling_factor = config.get_synthetic_output_size() / config.get_current_dataset_size()\n",
    "    print(f\"Scaling Factor: {scaling_factor:.1f}x\")\n",
    "print(f\"Privacy Mode: {config.PRIVACY_MODE}\")\n",
    "print(f\"VAE Architecture: {config.ENCODER_LAYERS} → {config.LATENT_DIM} → {config.DECODER_LAYERS}\")\n",
    "print(f\"Training: {config.EPOCHS} epochs, batch size {config.BATCH_SIZE}\")\n",
    "print(f\"Quality Target: {config.STATISTICAL_MATCH_RATIO*100:.0f}% statistical match, {config.EDGE_CASE_RATIO*100:.0f}% edge cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Load Your Actual Data\n",
    "# REPLACE THIS SECTION WITH YOUR DATA LOADING CODE\n",
    "\n",
    "# ===========================================\n",
    "# OPTION 1: Load from CSV file\n",
    "# ===========================================\n",
    "# Uncomment and modify the path to your data file:\n",
    "# original_data = pd.read_csv('path/to/your/financial_data.csv')\n",
    "\n",
    "# ===========================================\n",
    "# OPTION 2: Load from Databricks table\n",
    "# ===========================================\n",
    "# Uncomment and modify the table name:\n",
    "# original_data = spark.table('your_database.your_table_name').toPandas()\n",
    "\n",
    "# ===========================================\n",
    "# OPTION 3: Load from uploaded file in Databricks\n",
    "# ===========================================\n",
    "# Uncomment and modify the file path:\n",
    "# original_data = pd.read_csv('/dbfs/FileStore/shared_uploads/your_email/your_file.csv')\n",
    "\n",
    "# ===========================================\n",
    "# OPTION 4: For testing - use sample data first\n",
    "# ===========================================\n",
    "# If you want to test with sample data first, uncomment this:\n",
    "print(\"IMPORTANT: Replace this section with your actual data loading code!\")\n",
    "print(\"This is currently using sample data for demonstration.\")\n",
    "print(\"\")\n",
    "print(\"To use your actual data:\")\n",
    "print(\"1. Upload your CSV file to Databricks\")\n",
    "print(\"2. Uncomment one of the data loading options above\")\n",
    "print(\"3. Comment out the sample data generation below\")\n",
    "print(\"\")\n",
    "\n",
    "# TEMPORARY SAMPLE DATA (REMOVE THIS WHEN USING REAL DATA)\n",
    "# This creates data matching your exact schema for testing\n",
    "print(\"Creating sample data matching your schema...\")\n",
    "\n",
    "# Set size based on configuration\n",
    "size = config.get_current_dataset_size()\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Create sample data matching your exact column structure\n",
    "companies = [\n",
    "    'Chevron Corporation', 'Capital One Financial Corporation', 'CBIZ Inc',\n",
    "    'Shift4 Payments Inc', 'Automatic Data Processing Inc', 'SI-BONE Inc',\n",
    "    'United Parcel Service Inc', 'The PNC Financial Services Group Inc'\n",
    "]\n",
    "\n",
    "industries = ['Energy', 'Financial', 'Industrials', 'Technology', 'Health Care']\n",
    "gics = ['Energy', 'Financials', 'Industrials', 'Technology', 'Healthcare']\n",
    "subindustries = [\n",
    "    'Energy', 'Banks', 'Transportation', 'Financial Services',\n",
    "    'Commercial Services & Supplies', 'Health Care Equipment & Supplies'\n",
    "]\n",
    "\n",
    "# Generate realistic dates and times for better temporal analysis\n",
    "start_date = 250400  # YYMMDD format\n",
    "date_range = 200\n",
    "dates = np.random.randint(start_date, start_date + date_range, size)\n",
    "\n",
    "# Generate times with realistic business hour patterns\n",
    "business_hours = list(range(800, 1800))  # 8 AM to 6 PM\n",
    "after_hours = list(range(0, 800)) + list(range(1800, 2400))\n",
    "times = np.random.choice(business_hours, int(size * 0.8)).tolist() + np.random.choice(after_hours, int(size * 0.2)).tolist()\n",
    "times = np.random.choice(times, size)\n",
    "\n",
    "original_data = pd.DataFrame({\n",
    "    'payer_Company_Name': np.random.choice(companies, size),\n",
    "    'payee_Company_Name': np.random.choice(companies, size),\n",
    "    'payer_industry': np.random.choice(industries, size),\n",
    "    'payee_industry': np.random.choice(industries, size),\n",
    "    'payer_GICS': np.random.choice(gics, size),\n",
    "    'payee_GICS': np.random.choice(gics, size),\n",
    "    'payer_subindustry': np.random.choice(subindustries, size),\n",
    "    'payee_subindustry': np.random.choice(subindustries, size),\n",
    "    'ed_amount': np.random.lognormal(mean=7, sigma=1.5, size=size),\n",
    "    'fh_file_creation_date': dates,\n",
    "    'fh_file_creation_time': times\n",
    "})\n",
    "\n",
    "# Apply business constraints\n",
    "original_data['ed_amount'] = np.clip(original_data['ed_amount'], 0.01, 1000000.0)\n",
    "\n",
    "print(f\"Data loaded: {len(original_data):,} rows\")\n",
    "print(f\"Columns: {list(original_data.columns)}\")\n",
    "\n",
    "# ===========================================\n",
    "# DATA VALIDATION\n",
    "# ===========================================\n",
    "print(\"\\nValidating data structure...\")\n",
    "\n",
    "# Check required columns\n",
    "required_columns = config.CATEGORICAL_COLUMNS + config.NUMERICAL_COLUMNS\n",
    "missing_columns = [col for col in required_columns if col not in original_data.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"WARNING: Missing columns: {missing_columns}\")\n",
    "    print(\"Please ensure your data has all required columns.\")\n",
    "else:\n",
    "    print(\"✓ All required columns present\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData validation summary:\")\n",
    "for col in config.CATEGORICAL_COLUMNS:\n",
    "    if col in original_data.columns:\n",
    "        unique_vals = original_data[col].nunique()\n",
    "        print(f\"  {col}: {unique_vals} unique values\")\n",
    "\n",
    "for col in config.NUMERICAL_COLUMNS:\n",
    "    if col in original_data.columns:\n",
    "        min_val = original_data[col].min()\n",
    "        max_val = original_data[col].max()\n",
    "        print(f\"  {col}: Range {min_val:.2f} to {max_val:.2f}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Data Preview:\")\n",
    "display(original_data.head(10))\n",
    "\n",
    "print(\"\\nData Summary:\")\n",
    "display(original_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Data Preprocessing for Your Actual Data\n",
    "\n",
    "class FinancialDataProcessor:\n",
    "    \"\"\"Data preprocessing for financial transaction data.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.label_encoders = {}\n",
    "        self.numerical_scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "        self.feature_dim = 0\n",
    "    \n",
    "    def fit(self, data: pd.DataFrame) -> 'FinancialDataProcessor':\n",
    "        \"\"\"Fit preprocessing transformers on your actual data.\"\"\"\n",
    "        print(\"Fitting preprocessing transformers on your data...\")\n",
    "        \n",
    "        # Validate input data\n",
    "        self._validate_data(data)\n",
    "        \n",
    "        # Fit label encoders for categorical columns\n",
    "        for col in self.config.CATEGORICAL_COLUMNS:\n",
    "            if col in data.columns:\n",
    "                # Handle missing values\n",
    "                data[col] = data[col].fillna('Unknown')\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(data[col].astype(str))\n",
    "                self.label_encoders[col] = encoder\n",
    "                print(f\"  {col}: {len(encoder.classes_)} unique values\")\n",
    "        \n",
    "        # Fit numerical scaler\n",
    "        numerical_data = data[self.config.NUMERICAL_COLUMNS]\n",
    "        # Handle missing values\n",
    "        numerical_data = numerical_data.fillna(numerical_data.mean())\n",
    "        self.numerical_scaler.fit(numerical_data)\n",
    "        \n",
    "        # Calculate feature dimensions\n",
    "        categorical_dims = sum(len(encoder.classes_) for encoder in self.label_encoders.values())\n",
    "        numerical_dims = len(self.config.NUMERICAL_COLUMNS)\n",
    "        self.feature_dim = categorical_dims + numerical_dims\n",
    "        \n",
    "        self.fitted = True\n",
    "        print(f\"\\nPreprocessing fitted successfully:\")\n",
    "        print(f\"  Categorical features: {categorical_dims}\")\n",
    "        print(f\"  Numerical features: {numerical_dims}\")\n",
    "        print(f\"  Total features: {self.feature_dim}\")\n",
    "        return self\n",
    "    \n",
    "    def _validate_data(self, data: pd.DataFrame):\n",
    "        \"\"\"Validate data structure and content.\"\"\"\n",
    "        # Check required columns\n",
    "        required_cols = self.config.CATEGORICAL_COLUMNS + self.config.NUMERICAL_COLUMNS\n",
    "        missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Check data types and ranges\n",
    "        for col in self.config.NUMERICAL_COLUMNS:\n",
    "            if col in data.columns:\n",
    "                if not pd.api.types.is_numeric_dtype(data[col]):\n",
    "                    print(f\"WARNING: {col} is not numeric. Converting...\")\n",
    "                    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "        \n",
    "        print(\"Data validation passed\")\n",
    "    \n",
    "    def transform(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Transform data using fitted encoders.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Processor must be fitted before transform\")\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Encode categorical columns using one-hot encoding\n",
    "        for col in self.config.CATEGORICAL_COLUMNS:\n",
    "            if col in data.columns:\n",
    "                # Handle missing values\n",
    "                col_data = data[col].fillna('Unknown').astype(str)\n",
    "                \n",
    "                # Handle unseen categories\n",
    "                encoder = self.label_encoders[col]\n",
    "                encoded = []\n",
    "                for val in col_data:\n",
    "                    if val in encoder.classes_:\n",
    "                        encoded.append(encoder.transform([val])[0])\n",
    "                    else:\n",
    "                        # Assign to first class for unseen values\n",
    "                        encoded.append(0)\n",
    "                \n",
    "                encoded = np.array(encoded)\n",
    "                one_hot = np.eye(len(encoder.classes_))[encoded]\n",
    "                features.append(one_hot)\n",
    "        \n",
    "        # Scale numerical columns\n",
    "        numerical_data = data[self.config.NUMERICAL_COLUMNS].fillna(data[self.config.NUMERICAL_COLUMNS].mean())\n",
    "        scaled_numerical = self.numerical_scaler.transform(numerical_data)\n",
    "        features.append(scaled_numerical)\n",
    "        \n",
    "        return np.concatenate(features, axis=1)\n",
    "    \n",
    "    def inverse_transform(self, transformed_data: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"Convert VAE output back to original format.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Processor must be fitted before inverse transform\")\n",
    "        \n",
    "        result_data = {}\n",
    "        feature_idx = 0\n",
    "        \n",
    "        # Decode categorical columns\n",
    "        for col in self.config.CATEGORICAL_COLUMNS:\n",
    "            if col in self.label_encoders:\n",
    "                num_classes = len(self.label_encoders[col].classes_)\n",
    "                one_hot_data = transformed_data[:, feature_idx:feature_idx + num_classes]\n",
    "                categorical_indices = np.argmax(one_hot_data, axis=1)\n",
    "                result_data[col] = self.label_encoders[col].inverse_transform(categorical_indices)\n",
    "                feature_idx += num_classes\n",
    "        \n",
    "        # Inverse scale numerical columns\n",
    "        numerical_data = transformed_data[:, feature_idx:feature_idx + len(self.config.NUMERICAL_COLUMNS)]\n",
    "        scaled_back = self.numerical_scaler.inverse_transform(numerical_data)\n",
    "        \n",
    "        for i, col in enumerate(self.config.NUMERICAL_COLUMNS):\n",
    "            result_data[col] = scaled_back[:, i]\n",
    "            \n",
    "            # Apply business constraints\n",
    "            if col in self.config.COLUMN_RANGES:\n",
    "                result_data[col] = np.clip(\n",
    "                    result_data[col],\n",
    "                    self.config.COLUMN_RANGES[col]['min'],\n",
    "                    self.config.COLUMN_RANGES[col]['max']\n",
    "                )\n",
    "        \n",
    "        return pd.DataFrame(result_data)\n",
    "    \n",
    "    def fit_transform(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        return self.fit(data).transform(data)\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"Get total feature dimensions.\"\"\"\n",
    "        return self.feature_dim\n",
    "\n",
    "# Initialize data processor with your actual data\n",
    "processor = FinancialDataProcessor(config)\n",
    "\n",
    "# Fit the processor to your data\n",
    "print(\"Processing your financial transaction data...\")\n",
    "transformed_data = processor.fit_transform(original_data)\n",
    "\n",
    "print(f\"\\nData preprocessing completed:\")\n",
    "print(f\"  Original shape: {original_data.shape}\")\n",
    "print(f\"  Transformed shape: {transformed_data.shape}\")\n",
    "print(f\"  Ready for VAE training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: VAE Model Architecture\n",
    "\n",
    "class FinancialVAE(tf.keras.Model):\n",
    "    \"\"\"Variational Autoencoder for financial transaction data.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = config.LATENT_DIM\n",
    "        self.beta = config.BETA_KL\n",
    "        \n",
    "        # Build encoder\n",
    "        self.encoder = self._build_encoder()\n",
    "        \n",
    "        # Build decoder\n",
    "        self.decoder = self._build_decoder()\n",
    "        \n",
    "        print(f\"VAE Architecture initialized:\")\n",
    "        print(f\"  Input dimension: {input_dim}\")\n",
    "        print(f\"  Latent dimension: {config.LATENT_DIM}\")\n",
    "        print(f\"  Encoder layers: {config.ENCODER_LAYERS}\")\n",
    "        print(f\"  Decoder layers: {config.DECODER_LAYERS}\")\n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        \"\"\"Build encoder network.\"\"\"\n",
    "        encoder_inputs = layers.Input(shape=(self.input_dim,))\n",
    "        x = encoder_inputs\n",
    "        \n",
    "        # Hidden layers\n",
    "        for units in self.config.ENCODER_LAYERS:\n",
    "            x = layers.Dense(units, activation=self.config.ACTIVATION)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Latent space parameters\n",
    "        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)\n",
    "        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)\n",
    "        \n",
    "        return models.Model(encoder_inputs, [z_mean, z_log_var], name='encoder')\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        \"\"\"Build decoder network.\"\"\"\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,))\n",
    "        x = latent_inputs\n",
    "        \n",
    "        # Hidden layers\n",
    "        for units in self.config.DECODER_LAYERS:\n",
    "            x = layers.Dense(units, activation=self.config.ACTIVATION)(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_outputs = layers.Dense(self.input_dim, activation='sigmoid')(x)\n",
    "        \n",
    "        return models.Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        \"\"\"Reparameterization trick.\"\"\"\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Forward pass through VAE.\"\"\"\n",
    "        z_mean, z_log_var = self.encoder(inputs, training=training)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstruction = self.decoder(z, training=training)\n",
    "        \n",
    "        # Add KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "        self.add_loss(self.beta * kl_loss)\n",
    "        \n",
    "        return reconstruction\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode data to latent space.\"\"\"\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        return self.reparameterize(z_mean, z_log_var)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode from latent space.\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def generate_synthetic_data(self, num_samples: int) -> np.ndarray:\n",
    "        \"\"\"Generate synthetic data samples.\"\"\"\n",
    "        # Sample from latent space\n",
    "        z_samples = tf.random.normal(shape=(num_samples, self.latent_dim))\n",
    "        \n",
    "        # Decode to generate synthetic data\n",
    "        synthetic_data = self.decoder(z_samples, training=False)\n",
    "        \n",
    "        return synthetic_data.numpy()\n",
    "\n",
    "# Initialize VAE model\n",
    "vae = FinancialVAE(config, processor.get_feature_dim())\n",
    "\n",
    "# Compile model\n",
    "vae.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\nVAE model compiled and ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Train VAE Model\n",
    "\n",
    "print(\"Starting VAE training...\")\n",
    "print(f\"Training parameters:\")\n",
    "print(f\"  Epochs: {config.EPOCHS}\")\n",
    "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Beta (KL weight): {config.BETA_KL}\")\n",
    "\n",
    "# Prepare training data\n",
    "train_data = transformed_data.astype(np.float32)\n",
    "\n",
    "# Training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "history = vae.fit(\n",
    "    train_data,\n",
    "    train_data,  # VAE reconstructs its input\n",
    "    epochs=config.EPOCHS,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = datetime.now() - start_time\n",
    "print(f\"\\nTraining completed in: {training_time}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history.history['loss'], label='Training Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('VAE Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# MAE plot\n",
    "ax2.plot(history.history['mae'], label='Training MAE')\n",
    "ax2.plot(history.history['val_mae'], label='Validation MAE')\n",
    "ax2.set_title('VAE Mean Absolute Error')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"VAE training completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: Generate Synthetic Data with Configurable Scaling\n",
    "\n",
    "# ===========================================\n",
    "# SCALING CONFIGURATION (MODIFY FOR FUTURE SCALING)\n",
    "# ===========================================\n",
    "# For immediate work: Keep these settings\n",
    "ENABLE_ADVANCED_SCALING = config.ENABLE_ADVANCED_SCALING\n",
    "CUSTOM_OUTPUT_SIZE = config.CUSTOM_OUTPUT_SIZE\n",
    "BATCH_GENERATION = config.BATCH_GENERATION\n",
    "GENERATION_BATCH_SIZE = config.GENERATION_BATCH_SIZE\n",
    "\n",
    "# ===========================================\n",
    "# DETERMINE OUTPUT SIZE\n",
    "# ===========================================\n",
    "target_size = config.get_synthetic_output_size()\n",
    "\n",
    "if ENABLE_ADVANCED_SCALING:\n",
    "    print(f\"ADVANCED SCALING ENABLED: Generating {target_size:,} synthetic rows\")\n",
    "    scaling_factor = target_size / len(original_data)\n",
    "    print(f\"Scaling factor: {scaling_factor:.1f}x\")\n",
    "else:\n",
    "    print(f\"STANDARD MODE: Generating {target_size:,} synthetic rows (same as training size)\")\n",
    "\n",
    "print(\"\\nSynthetic Data Generation Parameters:\")\n",
    "print(f\"  Statistical Match Target: {config.STATISTICAL_MATCH_RATIO*100:.0f}%\")\n",
    "print(f\"  Edge Case Generation: {config.EDGE_CASE_RATIO*100:.0f}%\")\n",
    "print(f\"  Original dataset size: {len(original_data):,} rows\")\n",
    "print(f\"  Target synthetic size: {target_size:,} rows\")\n",
    "\n",
    "# Calculate generation split based on configured ratios\n",
    "statistical_samples = int(target_size * config.STATISTICAL_MATCH_RATIO)\n",
    "edge_case_samples = int(target_size * config.EDGE_CASE_RATIO)\n",
    "\n",
    "print(f\"\\nGeneration Strategy:\")\n",
    "print(f\"  Statistical similarity samples: {statistical_samples:,} ({config.STATISTICAL_MATCH_RATIO*100:.0f}%)\")\n",
    "print(f\"  Edge case samples: {edge_case_samples:,} ({config.EDGE_CASE_RATIO*100:.0f}%)\")\n",
    "print(f\"  Total synthetic samples: {statistical_samples + edge_case_samples:,}\")\n",
    "\n",
    "# ===========================================\n",
    "# GENERATE SYNTHETIC DATA\n",
    "# ===========================================\n",
    "if BATCH_GENERATION and target_size > GENERATION_BATCH_SIZE:\n",
    "    print(f\"\\nUsing batch generation ({GENERATION_BATCH_SIZE:,} rows per batch)...\")\n",
    "    \n",
    "    # Generate in batches\n",
    "    synthetic_batches = []\n",
    "    remaining_statistical = statistical_samples\n",
    "    remaining_edge = edge_case_samples\n",
    "    \n",
    "    batch_num = 1\n",
    "    while remaining_statistical > 0 or remaining_edge > 0:\n",
    "        batch_statistical = min(remaining_statistical, int(GENERATION_BATCH_SIZE * config.STATISTICAL_MATCH_RATIO))\n",
    "        batch_edge = min(remaining_edge, int(GENERATION_BATCH_SIZE * config.EDGE_CASE_RATIO))\n",
    "        \n",
    "        if batch_statistical > 0:\n",
    "            stat_batch = vae.generate_synthetic_data(batch_statistical)\n",
    "            synthetic_batches.append(stat_batch)\n",
    "            remaining_statistical -= batch_statistical\n",
    "        \n",
    "        if batch_edge > 0:\n",
    "            z_edge = tf.random.normal(shape=(batch_edge, config.LATENT_DIM), stddev=2.0)\n",
    "            edge_batch = vae.decode(z_edge).numpy()\n",
    "            synthetic_batches.append(edge_batch)\n",
    "            remaining_edge -= batch_edge\n",
    "        \n",
    "        print(f\"  Batch {batch_num} completed: {batch_statistical + batch_edge:,} rows\")\n",
    "        batch_num += 1\n",
    "    \n",
    "    # Combine all batches\n",
    "    combined_synthetic = np.vstack(synthetic_batches)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nGenerating synthetic data in single batch...\")\n",
    "    \n",
    "    # Generate statistical similarity data (90%)\n",
    "    print(\"Generating statistical similarity data...\")\n",
    "    statistical_synthetic = vae.generate_synthetic_data(statistical_samples)\n",
    "    \n",
    "    # Generate edge case data (10%)\n",
    "    print(\"Generating edge case data...\")\n",
    "    z_edge = tf.random.normal(shape=(edge_case_samples, config.LATENT_DIM), stddev=2.0)\n",
    "    edge_case_synthetic = vae.decode(z_edge).numpy()\n",
    "    \n",
    "    # Combine synthetic data\n",
    "    combined_synthetic = np.vstack([statistical_synthetic, edge_case_synthetic])\n",
    "\n",
    "# ===========================================\n",
    "# CONVERT BACK TO ORIGINAL FORMAT\n",
    "# ===========================================\n",
    "print(\"Converting synthetic data back to original format...\")\n",
    "synthetic_data = processor.inverse_transform(combined_synthetic)\n",
    "\n",
    "print(f\"\\nSynthetic data generation completed:\")\n",
    "print(f\"  Generated {len(synthetic_data):,} synthetic rows\")\n",
    "print(f\"  Scaling factor: {len(synthetic_data) / len(original_data):.1f}x\")\n",
    "print(f\"  Original columns: {list(original_data.columns)}\")\n",
    "print(f\"  Synthetic columns: {list(synthetic_data.columns)}\")\n",
    "\n",
    "# Display sample synthetic data\n",
    "print(\"\\nSample Synthetic Data:\")\n",
    "display(synthetic_data.head(10))\n",
    "\n",
    "print(\"\\nSynthetic Data Summary:\")\n",
    "display(synthetic_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: Multi-Dimensional Validation Framework\n",
    "\n",
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    \"\"\"Configuration for multi-dimensional validation.\"\"\"\n",
    "    \n",
    "    # ===========================================\n",
    "    # VALIDATION METRICS (Configurable)\n",
    "    # ===========================================\n",
    "    VALIDATION_METRICS: List[str] = field(default_factory=lambda: [\n",
    "        'transaction_amount',    # Sum of ed_amount\n",
    "        'transaction_count',     # Number of transactions\n",
    "        'avg_transaction'        # Average transaction amount\n",
    "    ])\n",
    "    \n",
    "    # ===========================================\n",
    "    # TEMPORAL VALIDATION PARAMETERS\n",
    "    # ===========================================\n",
    "    TIME_WINDOW_HOURS: int = 2              # Time aggregation window (2 or 4 hours)\n",
    "    ENABLE_TEMPORAL_VALIDATION: bool = True\n",
    "    \n",
    "    # ===========================================\n",
    "    # NETWORK VALIDATION PARAMETERS\n",
    "    # ===========================================\n",
    "    # Payer-centric validation\n",
    "    PAYER_VALIDATION_COUNT: int = 5         # Number of payers to analyze\n",
    "    PAYER_SELECTION_METHOD: str = 'random'  # 'random' or 'specific'\n",
    "    SPECIFIC_PAYERS: List[str] = field(default_factory=list)  # If using 'specific'\n",
    "    \n",
    "    # Payee-centric validation\n",
    "    PAYEE_VALIDATION_COUNT: int = 5         # Number of payees to analyze\n",
    "    PAYEE_SELECTION_METHOD: str = 'random'  # 'random' or 'specific'\n",
    "    SPECIFIC_PAYEES: List[str] = field(default_factory=list)  # If using 'specific'\n",
    "    \n",
    "    # Time frame for network analysis\n",
    "    NETWORK_ANALYSIS_DAYS: Optional[int] = 7  # Subset time window (None = full dataset)\n",
    "    \n",
    "    # ===========================================\n",
    "    # RELATIONSHIP VALIDATION LEVELS\n",
    "    # ===========================================\n",
    "    VALIDATE_COMPANY_LEVEL: bool = True     # payer_Company_Name ↔ payee_Company_Name\n",
    "    VALIDATE_GICS_LEVEL: bool = True        # payer_GICS ↔ payee_GICS\n",
    "    VALIDATE_INDUSTRY_LEVEL: bool = True    # payer_industry ↔ payee_industry\n",
    "    \n",
    "    # ===========================================\n",
    "    # STATISTICAL SIMILARITY THRESHOLDS\n",
    "    # ===========================================\n",
    "    SIMILARITY_THRESHOLD: float = 0.85      # Minimum similarity score\n",
    "    KS_TEST_THRESHOLD: float = 0.05         # Kolmogorov-Smirnov p-value threshold\n",
    "    CORRELATION_THRESHOLD: float = 0.1      # Maximum correlation difference\n",
    "    \n",
    "    # ===========================================\n",
    "    # ROW-LEVEL SAMPLING FOR SANITY CHECKS\n",
    "    # ===========================================\n",
    "    ROW_SAMPLE_SIZE: int = 10                # Number of rows to sample for sanity checks (M parameter)\n",
    "    ENABLE_ROW_LEVEL_VALIDATION: bool = True # Enable row-level sampling validation\n",
    "    \n",
    "    # ===========================================\n",
    "    # STATISTICAL SUMMARY CONFIGURATION\n",
    "    # ===========================================\n",
    "    STATISTICAL_SAMPLE_SIZE: int = 3500      # Sample size for statistical analysis (N parameter)\n",
    "    ENABLE_STATISTICAL_SUMMARY: bool = True  # Enable comprehensive statistical summaries\n",
    "    STATISTICAL_METRICS: List[str] = field(default_factory=lambda: [\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max', '25%', '50%', '75%'\n",
    "    ])"\n",
    "\n",
    "class MultiDimensionalValidator:\n",
    "    \"\"\"Comprehensive validation across temporal, network, and industry dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_config: ValidationConfig = None):\n",
    "        self.config = validation_config or ValidationConfig()\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def run_comprehensive_validation(self, original_data: pd.DataFrame, \n",
    "                                   synthetic_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Run all validation dimensions.\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"MULTI-DIMENSIONAL VALIDATION FRAMEWORK\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {\n",
    "            'temporal_validation': {},\n",
    "            'network_validation': {},\n",
    "            'industry_validation': {},\n",
    "            'entity_distribution_validation': {},\n",
    "            'overall_scores': {}\n",
    "        }\n",
    "        \n",
    "        # 1. Temporal Validation\n",
    "        if self.config.ENABLE_TEMPORAL_VALIDATION:\n",
    "            print(f\"\\n1. TEMPORAL VALIDATION ({self.config.TIME_WINDOW_HOURS}-hour windows)\")\n",
    "            results['temporal_validation'] = self.validate_temporal_patterns(\n",
    "                original_data, synthetic_data\n",
    "            )\n",
    "        \n",
    "        # 2. Network Validation - Company Level\n",
    "        if self.config.VALIDATE_COMPANY_LEVEL:\n",
    "            print(\"\\n2. COMPANY-LEVEL NETWORK VALIDATION\")\n",
    "            results['network_validation']['company'] = self.validate_network_patterns(\n",
    "                original_data, synthetic_data, \n",
    "                'payer_Company_Name', 'payee_Company_Name', 'Company'\n",
    "            )\n",
    "        \n",
    "        # 3. Network Validation - GICS Level\n",
    "        if self.config.VALIDATE_GICS_LEVEL:\n",
    "            print(\"\\n3. GICS-LEVEL NETWORK VALIDATION\")\n",
    "            results['network_validation']['gics'] = self.validate_network_patterns(\n",
    "                original_data, synthetic_data,\n",
    "                'payer_GICS', 'payee_GICS', 'GICS'\n",
    "            )\n",
    "        \n",
    "        # 4. Network Validation - Industry Level\n",
    "        if self.config.VALIDATE_INDUSTRY_LEVEL:\n",
    "            print(\"\\n4. INDUSTRY-LEVEL NETWORK VALIDATION\")\n",
    "            results['network_validation']['industry'] = self.validate_network_patterns(\n",
    "                original_data, synthetic_data,\n",
    "                'payer_industry', 'payee_industry', 'Industry'\n",
    "            )\n",
    "        \n",
    "        # 5. Entity Distribution Validation\n",
    "        print(\"\\n5. ENTITY DISTRIBUTION VALIDATION\")\n",
    "        results['entity_distribution_validation'] = self.validate_entity_distributions(\n",
    "            original_data, synthetic_data\n",
    "        )\n",
    "        \n",
    "        # 6. Row-Level Sampling for Sanity Checks
        if self.config.ENABLE_ROW_LEVEL_VALIDATION:
            print(f\"\\n6. ROW-LEVEL SANITY CHECKS (M={self.config.ROW_SAMPLE_SIZE})\")
            results['row_level_validation'] = self.validate_row_level_sampling(
                original_data, synthetic_data
            )
        
        # 7. Statistical Summary Comparison
        if self.config.ENABLE_STATISTICAL_SUMMARY:
            print(f\"\\n7. STATISTICAL SUMMARY COMPARISON (N={self.config.STATISTICAL_SAMPLE_SIZE})\")
            results['statistical_summary'] = self.generate_statistical_summary(
                original_data, synthetic_data
            )
        
        # 8. Calculate Overall Scores\n",
    "        results['overall_scores'] = self.calculate_overall_scores(results)\n",
    "        \n",
    "        # 7. Generate Summary Report\n",
    "        self.generate_validation_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate_temporal_patterns(self, original_data: pd.DataFrame, \n",
    "                                 synthetic_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Validate temporal aggregation patterns.\"\"\"\n",
    "        # Convert time to datetime for aggregation\n",
    "        def prepare_temporal_data(data):\n",
    "            df = data.copy()\n",
    "            # Create datetime from date and time\n",
    "            df['datetime'] = pd.to_datetime(\n",
    "                df['fh_file_creation_date'].astype(str) + ' ' + \n",
    "                df['fh_file_creation_time'].astype(str).str.zfill(4),\n",
    "                format='%y%m%d %H%M',\n",
    "                errors='coerce'\n",
    "            )\n",
    "            return df.dropna(subset=['datetime'])\n",
    "        \n",
    "        orig_temporal = prepare_temporal_data(original_data)\n",
    "        synth_temporal = prepare_temporal_data(synthetic_data)\n",
    "        \n",
    "        if orig_temporal.empty or synth_temporal.empty:\n",
    "            return {'error': 'Invalid datetime conversion'}\n",
    "        \n",
    "        # Group by time windows\n",
    "        freq = f'{self.config.TIME_WINDOW_HOURS}H'\n",
    "        \n",
    "        def aggregate_by_time_window(df):\n",
    "            df_grouped = df.set_index('datetime').groupby(pd.Grouper(freq=freq)).agg({\n",
    "                'ed_amount': ['sum', 'count', 'mean']\n",
    "            }).round(2)\n",
    "            df_grouped.columns = ['transaction_amount', 'transaction_count', 'avg_transaction']\n",
    "            return df_grouped.dropna()\n",
    "        \n",
    "        orig_agg = aggregate_by_time_window(orig_temporal)\n",
    "        synth_agg = aggregate_by_time_window(synth_temporal)\n",
    "        \n",
    "        # Statistical comparison\n",
    "        temporal_results = {}\n",
    "        \n",
    "        for metric in self.config.VALIDATION_METRICS:\n",
    "            if metric in orig_agg.columns and metric in synth_agg.columns:\n",
    "                orig_values = orig_agg[metric].dropna()\n",
    "                synth_values = synth_agg[metric].dropna()\n",
    "                \n",
    "                # Statistical tests\n",
    "                ks_stat, ks_p = stats.ks_2samp(orig_values, synth_values)\n",
    "                \n",
    "                # Distribution comparison\n",
    "                mean_diff = abs(orig_values.mean() - synth_values.mean()) / orig_values.mean() if orig_values.mean() != 0 else 0\n",
    "                std_diff = abs(orig_values.std() - synth_values.std()) / orig_values.std() if orig_values.std() != 0 else 0\n",
    "                \n",
    "                temporal_results[metric] = {\n",
    "                    'ks_statistic': ks_stat,\n",
    "                    'ks_p_value': ks_p,\n",
    "                    'mean_difference_pct': mean_diff * 100,\n",
    "                    'std_difference_pct': std_diff * 100,\n",
    "                    'similarity_score': (1 - ks_stat) * (1 - min(mean_diff, 1)) * (1 - min(std_diff, 1)),\n",
    "                    'original_stats': {\n",
    "                        'mean': orig_values.mean(),\n",
    "                        'std': orig_values.std(),\n",
    "                        'count': len(orig_values)\n",
    "                    },\n",
    "                    'synthetic_stats': {\n",
    "                        'mean': synth_values.mean(),\n",
    "                        'std': synth_values.std(),\n",
    "                        'count': len(synth_values)\n",
    "                    }\n",
    "                }\n",
    "        \n",
    "        # Store aggregated data for visualization\n",
    "        temporal_results['original_aggregated'] = orig_agg\n",
    "        temporal_results['synthetic_aggregated'] = synth_agg\n",
    "        temporal_results['time_window_hours'] = self.config.TIME_WINDOW_HOURS\n",
    "        \n",
    "        return temporal_results\n",
    "    \n",
    "    def validate_network_patterns(self, original_data: pd.DataFrame, \n",
    "                                synthetic_data: pd.DataFrame,\n",
    "                                payer_col: str, payee_col: str, \n",
    "                                level_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate network flow patterns at specified relationship level.\"\"\"\n",
    "        \n",
    "        def analyze_network_flows(data, analysis_type='payer_centric'):\n",
    "            if analysis_type == 'payer_centric':\n",
    "                group_col, target_col = payer_col, payee_col\n",
    "                selection_method = self.config.PAYER_SELECTION_METHOD\n",
    "                count = self.config.PAYER_VALIDATION_COUNT\n",
    "                specific_entities = self.config.SPECIFIC_PAYERS\n",
    "            else:  # payee_centric\n",
    "                group_col, target_col = payee_col, payer_col\n",
    "                selection_method = self.config.PAYEE_SELECTION_METHOD\n",
    "                count = self.config.PAYEE_VALIDATION_COUNT\n",
    "                specific_entities = self.config.SPECIFIC_PAYEES\n",
    "            \n",
    "            # Select entities to analyze\n",
    "            if selection_method == 'specific' and specific_entities:\n",
    "                selected_entities = [e for e in specific_entities if e in data[group_col].unique()]\n",
    "            else:\n",
    "                # Select top entities by transaction volume\n",
    "                entity_volumes = data.groupby(group_col)['ed_amount'].sum().sort_values(ascending=False)\n",
    "                selected_entities = entity_volumes.head(count).index.tolist()\n",
    "            \n",
    "            flows = {}\n",
    "            for entity in selected_entities:\n",
    "                entity_data = data[data[group_col] == entity]\n",
    "                \n",
    "                # Aggregate by target entities\n",
    "                flow_agg = entity_data.groupby(target_col).agg({\n",
    "                    'ed_amount': ['sum', 'count', 'mean']\n",
    "                }).round(2)\n",
    "                flow_agg.columns = self.config.VALIDATION_METRICS\n",
    "                \n",
    "                flows[entity] = flow_agg\n",
    "            \n",
    "            return flows, selected_entities\n",
    "        \n",
    "        # Filter data by time window if specified\n",
    "        if self.config.NETWORK_ANALYSIS_DAYS:\n",
    "            # Use most recent data within specified days\n",
    "            max_date = original_data['fh_file_creation_date'].max()\n",
    "            min_date = max_date - self.config.NETWORK_ANALYSIS_DAYS * 100  # Rough day conversion\n",
    "            \n",
    "            orig_filtered = original_data[original_data['fh_file_creation_date'] >= min_date]\n",
    "            synth_filtered = synthetic_data[synthetic_data['fh_file_creation_date'] >= min_date]\n",
    "        else:\n",
    "            orig_filtered = original_data\n",
    "            synth_filtered = synthetic_data\n",
    "        \n",
    "        network_results = {}\n",
    "        \n",
    "        # Payer-centric analysis\n",
    "        print(f\"   Analyzing {level_name} payer-centric flows...\")\n",
    "        orig_payer_flows, orig_payers = analyze_network_flows(orig_filtered, 'payer_centric')\n",
    "        synth_payer_flows, synth_payers = analyze_network_flows(synth_filtered, 'payer_centric')\n",
    "        \n",
    "        # Compare flows for common payers\n",
    "        common_payers = set(orig_payers) & set(synth_payers)\n",
    "        payer_similarities = self.compare_network_flows(orig_payer_flows, synth_payer_flows, common_payers)\n",
    "        \n",
    "        network_results['payer_centric'] = {\n",
    "            'analyzed_entities': list(common_payers),\n",
    "            'flow_similarities': payer_similarities,\n",
    "            'original_flows': {k: v for k, v in orig_payer_flows.items() if k in common_payers},\n",
    "            'synthetic_flows': {k: v for k, v in synth_payer_flows.items() if k in common_payers}\n",
    "        }\n",
    "        \n",
    "        # Payee-centric analysis\n",
    "        print(f\"   Analyzing {level_name} payee-centric flows...\")\n",
    "        orig_payee_flows, orig_payees = analyze_network_flows(orig_filtered, 'payee_centric')\n",
    "        synth_payee_flows, synth_payees = analyze_network_flows(synth_filtered, 'payee_centric')\n",
    "        \n",
    "        # Compare flows for common payees\n",
    "        common_payees = set(orig_payees) & set(synth_payees)\n",
    "        payee_similarities = self.compare_network_flows(orig_payee_flows, synth_payee_flows, common_payees)\n",
    "        \n",
    "        network_results['payee_centric'] = {\n",
    "            'analyzed_entities': list(common_payees),\n",
    "            'flow_similarities': payee_similarities,\n",
    "            'original_flows': {k: v for k, v in orig_payee_flows.items() if k in common_payees},\n",
    "            'synthetic_flows': {k: v for k, v in synth_payee_flows.items() if k in common_payees}\n",
    "        }\n",
    "        \n",
    "        return network_results\n",
    "    \n",
    "    def compare_network_flows(self, orig_flows: Dict, synth_flows: Dict, \n",
    "                            common_entities: set) -> Dict[str, Any]:\n",
    "        \"\"\"Compare network flows between original and synthetic data.\"\"\"\n",
    "        similarities = {}\n",
    "        \n",
    "        for entity in common_entities:\n",
    "            if entity in orig_flows and entity in synth_flows:\n",
    "                orig_flow = orig_flows[entity]\n",
    "                synth_flow = synth_flows[entity]\n",
    "                \n",
    "                entity_similarities = {}\n",
    "                \n",
    "                for metric in self.config.VALIDATION_METRICS:\n",
    "                    if metric in orig_flow.columns and metric in synth_flow.columns:\n",
    "                        orig_values = orig_flow[metric].dropna()\n",
    "                        synth_values = synth_flow[metric].dropna()\n",
    "                        \n",
    "                        if len(orig_values) > 0 and len(synth_values) > 0:\n",
    "                            # Distribution similarity\n",
    "                            ks_stat, ks_p = stats.ks_2samp(orig_values, synth_values)\n",
    "                            \n",
    "                            # Summary statistics comparison\n",
    "                            mean_diff = abs(orig_values.mean() - synth_values.mean()) / orig_values.mean() if orig_values.mean() != 0 else 0\n",
    "                            \n",
    "                            entity_similarities[metric] = {\n",
    "                                'ks_statistic': ks_stat,\n",
    "                                'ks_p_value': ks_p,\n",
    "                                'mean_difference_pct': mean_diff * 100,\n",
    "                                'similarity_score': 1 - ks_stat\n",
    "                            }\n",
    "                \n",
    "                similarities[entity] = entity_similarities\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def validate_entity_distributions(self, original_data: pd.DataFrame, \n",
    "                                    synthetic_data: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Validate transaction size distributions by entity type.\"\"\"\n",
    "        \n",
    "        distribution_results = {}\n",
    "        \n",
    "        # Analyze distributions by categorical dimensions\n",
    "        categorical_cols = ['payer_industry', 'payee_industry', 'payer_GICS', 'payee_GICS']\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in original_data.columns and col in synthetic_data.columns:\n",
    "                print(f\"   Analyzing transaction distributions by {col}...\")\n",
    "                \n",
    "                col_results = {}\n",
    "                \n",
    "                # Get common categories\n",
    "                orig_categories = set(original_data[col].unique())\n",
    "                synth_categories = set(synthetic_data[col].unique())\n",
    "                common_categories = orig_categories & synth_categories\n",
    "                \n",
    "                for category in common_categories:\n",
    "                    orig_amounts = original_data[original_data[col] == category]['ed_amount']\n",
    "                    synth_amounts = synthetic_data[synthetic_data[col] == category]['ed_amount']\n",
    "                    \n",
    "                    if len(orig_amounts) > 5 and len(synth_amounts) > 5:  # Minimum sample size\n",
    "                        # Distribution comparison\n",
    "                        ks_stat, ks_p = stats.ks_2samp(orig_amounts, synth_amounts)\n",
    "                        \n",
    "                        # Summary statistics\n",
    "                        mean_diff = abs(orig_amounts.mean() - synth_amounts.mean()) / orig_amounts.mean()\n",
    "                        std_diff = abs(orig_amounts.std() - synth_amounts.std()) / orig_amounts.std()\n",
    "                        \n",
    "                        col_results[category] = {\n",
    "                            'ks_statistic': ks_stat,\n",
    "                            'ks_p_value': ks_p,\n",
    "                            'mean_difference_pct': mean_diff * 100,\n",
    "                            'std_difference_pct': std_diff * 100,\n",
    "                            'similarity_score': (1 - ks_stat) * (1 - min(mean_diff, 1)),\n",
    "                            'sample_sizes': {\n",
    "                                'original': len(orig_amounts),\n",
    "                                'synthetic': len(synth_amounts)\n",
    "                            }\n",
    "                        }\n",
    "                \n",
    "                distribution_results[col] = col_results\n",
    "        \n",
    "        return distribution_results\n",
    "    \n",
    "    def calculate_overall_scores(self, results: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate overall validation scores across all dimensions.\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Temporal score\n",
    "        if 'temporal_validation' in results and results['temporal_validation']:\n",
    "            temporal_scores = []\n",
    "            for metric, metrics_data in results['temporal_validation'].items():\n",
    "                if isinstance(metrics_data, dict) and 'similarity_score' in metrics_data:\n",
    "                    temporal_scores.append(metrics_data['similarity_score'])\n",
    "            scores['temporal_average'] = np.mean(temporal_scores) if temporal_scores else 0.0\n",
    "        \n",
    "        # Network scores\n",
    "        network_scores = []\n",
    "        if 'network_validation' in results:\n",
    "            for level, level_data in results['network_validation'].items():\n",
    "                for analysis_type, type_data in level_data.items():\n",
    "                    if 'flow_similarities' in type_data:\n",
    "                        for entity, entity_sims in type_data['flow_similarities'].items():\n",
    "                            for metric, metric_data in entity_sims.items():\n",
    "                                if 'similarity_score' in metric_data:\n",
    "                                    network_scores.append(metric_data['similarity_score'])\n",
    "        scores['network_average'] = np.mean(network_scores) if network_scores else 0.0\n",
    "        \n",
    "        # Entity distribution scores\n",
    "        entity_scores = []\n",
    "        if 'entity_distribution_validation' in results:\n",
    "            for col, col_data in results['entity_distribution_validation'].items():\n",
    "                for category, category_data in col_data.items():\n",
    "                    if 'similarity_score' in category_data:\n",
    "                        entity_scores.append(category_data['similarity_score'])\n",
    "        scores['entity_distribution_average'] = np.mean(entity_scores) if entity_scores else 0.0\n",
    "        \n",
    "        # Overall composite score\n",
    "        all_scores = [score for score in scores.values() if score > 0]\n",
    "        scores['overall_composite'] = np.mean(all_scores) if all_scores else 0.0\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def generate_validation_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"Generate comprehensive validation summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"VALIDATION SUMMARY REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        overall_scores = results.get('overall_scores', {})\n",
    "        \n",
    "        print(f\"Overall Composite Score: {overall_scores.get('overall_composite', 0):.4f}/1.0\")\n",
    "        print(f\"Similarity Threshold: {self.config.SIMILARITY_THRESHOLD:.2f}\")\n",
    "        \n",
    "        composite_pass = overall_scores.get('overall_composite', 0) >= self.config.SIMILARITY_THRESHOLD\n",
    "        print(f\"Overall Validation Status: {'✓ PASS' if composite_pass else '✗ FAIL'}\")\n",
    "        \n",
    "        print(\"\\nDimensional Scores:\")\n",
    "        if 'temporal_average' in overall_scores:\n",
    "            score = overall_scores['temporal_average']\n",
    "            status = '✓ PASS' if score >= self.config.SIMILARITY_THRESHOLD else '✗ FAIL'\n",
    "            print(f\"  Temporal Patterns: {score:.4f} {status}\")\n",
    "        \n",
    "        if 'network_average' in overall_scores:\n",
    "            score = overall_scores['network_average']\n",
    "            status = '✓ PASS' if score >= self.config.SIMILARITY_THRESHOLD else '✗ FAIL'\n",
    "            print(f\"  Network Relationships: {score:.4f} {status}\")\n",
    "        \n",
    "        if 'entity_distribution_average' in overall_scores:\n",
    "            score = overall_scores['entity_distribution_average']\n",
    "            status = '✓ PASS' if score >= self.config.SIMILARITY_THRESHOLD else '✗ FAIL'\n",
    "            print(f\"  Entity Distributions: {score:.4f} {status}\")\n",
    "        \n",
    "        print(\"\\nValidation Configuration:\")\n",
    "        print(f\"  Time Window: {self.config.TIME_WINDOW_HOURS} hours\")\n",
    "        print(f\"  Network Analysis Window: {self.config.NETWORK_ANALYSIS_DAYS or 'Full dataset'} days\")\n",
    "        print(f\"  Validation Levels: \", end=\"\")\n",
    "        levels = []\n",
    "        if self.config.VALIDATE_COMPANY_LEVEL: levels.append('Company')\n",
    "        if self.config.VALIDATE_GICS_LEVEL: levels.append('GICS')\n",
    "        if self.config.VALIDATE_INDUSTRY_LEVEL: levels.append('Industry')\n",
    "        print(', '.join(levels))\n",
    "        \n",
    "        return composite_pass\n",
    "\n",
    "# Initialize validation framework\n",
    "validation_config = ValidationConfig()\n",
    "validator = MultiDimensionalValidator(validation_config)\n",
    "\n",
    "# Run comprehensive validation\n",
    "validation_results = validator.run_comprehensive_validation(original_data, synthetic_data)\n",
    "\n",
    "print(\"\\nMulti-dimensional validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: Visualization and Comparison Tables\n",
    "\n",
    "class ValidationVisualizer:\n",
    "    \"\"\"Generate comprehensive visualization and comparison tables.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_results: Dict[str, Any]):\n",
    "        self.results = validation_results\n",
    "    \n",
    "    def create_temporal_visualizations(self):\n",
    "        \"\"\"Create temporal pattern comparison charts.\"\"\"\n",
    "        if 'temporal_validation' not in self.results:\n",
    "            return\n",
    "        \n",
    "        temporal_data = self.results['temporal_validation']\n",
    "        \n",
    "        if 'original_aggregated' in temporal_data and 'synthetic_aggregated' in temporal_data:\n",
    "            orig_agg = temporal_data['original_aggregated']\n",
    "            synth_agg = temporal_data['synthetic_aggregated']\n",
    "            \n",
    "            # Create time series comparison plots\n",
    "            fig = make_subplots(\n",
    "                rows=3, cols=1,\n",
    "                subplot_titles=('Transaction Amount Over Time', \n",
    "                               'Transaction Count Over Time',\n",
    "                               'Average Transaction Over Time')\n",
    "            )\n",
    "            \n",
    "            # Transaction amount\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=orig_agg.index, y=orig_agg['transaction_amount'],\n",
    "                name='Original Amount', line=dict(color='blue')\n",
    "            ), row=1, col=1)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=synth_agg.index, y=synth_agg['transaction_amount'],\n",
    "                name='Synthetic Amount', line=dict(color='red', dash='dash')\n",
    "            ), row=1, col=1)\n",
    "            \n",
    "            # Transaction count\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=orig_agg.index, y=orig_agg['transaction_count'],\n",
    "                name='Original Count', line=dict(color='blue'), showlegend=False\n",
    "            ), row=2, col=1)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=synth_agg.index, y=synth_agg['transaction_count'],\n",
    "                name='Synthetic Count', line=dict(color='red', dash='dash'), showlegend=False\n",
    "            ), row=2, col=1)\n",
    "            \n",
    "            # Average transaction\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=orig_agg.index, y=orig_agg['avg_transaction'],\n",
    "                name='Original Avg', line=dict(color='blue'), showlegend=False\n",
    "            ), row=3, col=1)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=synth_agg.index, y=synth_agg['avg_transaction'],\n",
    "                name='Synthetic Avg', line=dict(color='red', dash='dash'), showlegend=False\n",
    "            ), row=3, col=1)\n",
    "            \n",
    "            window_hours = temporal_data.get('time_window_hours', 2)\n",
    "            fig.update_layout(\n",
    "                height=800,\n",
    "                title_text=f\"Temporal Pattern Comparison ({window_hours}-Hour Windows)\",\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Create comparison summary table\n",
    "            print(\"\\nTEMPORAL VALIDATION SUMMARY:\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            summary_data = []\n",
    "            for metric in ['transaction_amount', 'transaction_count', 'avg_transaction']:\n",
    "                if metric in temporal_data:\n",
    "                    metric_data = temporal_data[metric]\n",
    "                    summary_data.append({\n",
    "                        'Metric': metric.replace('_', ' ').title(),\n",
    "                        'Similarity Score': f\"{metric_data['similarity_score']:.4f}\",\n",
    "                        'Mean Diff %': f\"{metric_data['mean_difference_pct']:.2f}%\",\n",
    "                        'KS Statistic': f\"{metric_data['ks_statistic']:.4f}\",\n",
    "                        'KS P-Value': f\"{metric_data['ks_p_value']:.4f}\"\n",
    "                    })\n",
    "            \n",
    "            if summary_data:\n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                display(summary_df)\n",
    "    \n",
    "    def create_network_visualizations(self):\n",
    "        \"\"\"Create network flow comparison tables and charts.\"\"\"\n",
    "        if 'network_validation' not in self.results:\n",
    "            return\n",
    "        \n",
    "        network_data = self.results['network_validation']\n",
    "        \n",
    "        for level_name, level_data in network_data.items():\n",
    "            print(f\"\\n{level_name.upper()}-LEVEL NETWORK VALIDATION:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Payer-centric analysis\n",
    "            if 'payer_centric' in level_data:\n",
    "                payer_data = level_data['payer_centric']\n",
    "                analyzed_payers = payer_data.get('analyzed_entities', [])\n",
    "                \n",
    "                print(f\"\\nPayer-Centric Analysis ({len(analyzed_payers)} entities):\")\n",
    "                print(f\"Analyzed Payers: {', '.join(analyzed_payers[:3])}{'...' if len(analyzed_payers) > 3 else ''}\")\n",
    "                \n",
    "                # Create payer flow comparison table\n",
    "                payer_summary = []\n",
    "                for payer, similarities in payer_data.get('flow_similarities', {}).items():\n",
    "                    for metric, metric_data in similarities.items():\n",
    "                        payer_summary.append({\n",
    "                            'Payer': payer,\n",
    "                            'Metric': metric.replace('_', ' ').title(),\n",
    "                            'Similarity Score': f\"{metric_data['similarity_score']:.4f}\",\n",
    "                            'Mean Diff %': f\"{metric_data['mean_difference_pct']:.2f}%\",\n",
    "                            'KS Statistic': f\"{metric_data['ks_statistic']:.4f}\"\n",
    "                        })\n",
    "                \n",
    "                if payer_summary:\n",
    "                    payer_df = pd.DataFrame(payer_summary)\n",
    "                    display(payer_df.head(10))\n",
    "            \n",
    "            # Payee-centric analysis\n",
    "            if 'payee_centric' in level_data:\n",
    "                payee_data = level_data['payee_centric']\n",
    "                analyzed_payees = payee_data.get('analyzed_entities', [])\n",
    "                \n",
    "                print(f\"\\nPayee-Centric Analysis ({len(analyzed_payees)} entities):\")\n",
    "                print(f\"Analyzed Payees: {', '.join(analyzed_payees[:3])}{'...' if len(analyzed_payees) > 3 else ''}\")\n",
    "                \n",
    "                # Create payee flow comparison table\n",
    "                payee_summary = []\n",
    "                for payee, similarities in payee_data.get('flow_similarities', {}).items():\n",
    "                    for metric, metric_data in similarities.items():\n",
    "                        payee_summary.append({\n",
    "                            'Payee': payee,\n",
    "                            'Metric': metric.replace('_', ' ').title(),\n",
    "                            'Similarity Score': f\"{metric_data['similarity_score']:.4f}\",\n",
    "                            'Mean Diff %': f\"{metric_data['mean_difference_pct']:.2f}%\",\n",
    "                            'KS Statistic': f\"{metric_data['ks_statistic']:.4f}\"\n",
    "                        })\n",
    "                \n",
    "                if payee_summary:\n",
    "                    payee_df = pd.DataFrame(payee_summary)\n",
    "                    display(payee_df.head(10))\n",
    "    \n",
    "    def create_entity_distribution_visualizations(self):\n",
    "        \"\"\"Create entity distribution comparison charts.\"\"\"\n",
    "        if 'entity_distribution_validation' not in self.results:\n",
    "            return\n",
    "        \n",
    "        entity_data = self.results['entity_distribution_validation']\n",
    "        \n",
    "        print(\"\\nENTITY DISTRIBUTION VALIDATION:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for col_name, col_data in entity_data.items():\n",
    "            print(f\"\\n{col_name.replace('_', ' ').title()} Analysis:\")\n",
    "            \n",
    "            # Create distribution comparison table\n",
    "            dist_summary = []\n",
    "            for category, category_data in col_data.items():\n",
    "                dist_summary.append({\n",
    "                    'Category': category,\n",
    "                    'Similarity Score': f\"{category_data['similarity_score']:.4f}\",\n",
    "                    'Mean Diff %': f\"{category_data['mean_difference_pct']:.2f}%\",\n",
    "                    'Std Diff %': f\"{category_data['std_difference_pct']:.2f}%\",\n",
    "                    'KS Statistic': f\"{category_data['ks_statistic']:.4f}\",\n",
    "                    'Original Sample': category_data['sample_sizes']['original'],\n",
    "                    'Synthetic Sample': category_data['sample_sizes']['synthetic']\n",
    "                })\n",
    "            \n",
    "            if dist_summary:\n",
    "                dist_df = pd.DataFrame(dist_summary)\n",
    "                display(dist_df)\n",
    "    \n",
    "    def create_comprehensive_dashboard(self):\n",
    "        \"\"\"Create comprehensive validation dashboard.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"COMPREHENSIVE VALIDATION DASHBOARD\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Overall scores visualization\n",
    "        if 'overall_scores' in self.results:\n",
    "            scores = self.results['overall_scores']\n",
    "            \n",
    "            # Create radar chart for overall scores\n",
    "            categories = []\n",
    "            values = []\n",
    "            \n",
    "            if 'temporal_average' in scores:\n",
    "                categories.append('Temporal Patterns')\n",
    "                values.append(scores['temporal_average'])\n",
    "            \n",
    "            if 'network_average' in scores:\n",
    "                categories.append('Network Relationships')\n",
    "                values.append(scores['network_average'])\n",
    "            \n",
    "            if 'entity_distribution_average' in scores:\n",
    "                categories.append('Entity Distributions')\n",
    "                values.append(scores['entity_distribution_average'])\n",
    "            \n",
    "            if categories and values:\n",
    "                fig = go.Figure()\n",
    "                \n",
    "                fig.add_trace(go.Scatterpolar(\n",
    "                    r=values,\n",
    "                    theta=categories,\n",
    "                    fill='toself',\n",
    "                    name='Validation Scores',\n",
    "                    line_color='blue'\n",
    "                ))\n",
    "                \n",
    "                # Add threshold line\n",
    "                threshold_values = [0.85] * len(categories)\n",
    "                fig.add_trace(go.Scatterpolar(\n",
    "                    r=threshold_values,\n",
    "                    theta=categories,\n",
    "                    fill='toself',\n",
    "                    name='Threshold (0.85)',\n",
    "                    line_color='red',\n",
    "                    opacity=0.3\n",
    "                ))\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    polar=dict(\n",
    "                        radialaxis=dict(\n",
    "                            visible=True,\n",
    "                            range=[0, 1]\n",
    "                        )\n",
    "                    ),\n",
    "                    title=\"Multi-Dimensional Validation Scores\",\n",
    "                    showlegend=True\n",
    "                )\n",
    "                \n",
    "                fig.show()\n",
    "        \n",
    "        # Generate all visualizations\n",
    "        self.create_temporal_visualizations()\n",
    "        self.create_network_visualizations()\n",
    "        self.create_entity_distribution_visualizations()\n",
    "\n",
    "# Create comprehensive validation dashboard\n",
    "visualizer = ValidationVisualizer(validation_results)\n",
    "visualizer.create_comprehensive_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: Export and Final Report Generation\n",
    "\n",
    "class ValidationReporter:\n",
    "    \"\"\"Generate comprehensive validation reports and export data.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_results: Dict[str, Any], \n",
    "                 original_data: pd.DataFrame, synthetic_data: pd.DataFrame):\n",
    "        self.validation_results = validation_results\n",
    "        self.original_data = original_data\n",
    "        self.synthetic_data = synthetic_data\n",
    "    \n",
    "    def export_validation_data(self) -> Dict[str, str]:\n",
    "        \"\"\"Export validation data and comparison tables.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        exported_files = {}\n",
    "        \n",
    "        # Export original and synthetic data\n",
    "        orig_filename = f\"original_data_{timestamp}.csv\"\n",
    "        synth_filename = f\"synthetic_data_{timestamp}.csv\"\n",
    "        \n",
    "        self.original_data.to_csv(orig_filename, index=False)\n",
    "        self.synthetic_data.to_csv(synth_filename, index=False)\n",
    "        \n",
    "        exported_files['original_data'] = orig_filename\n",
    "        exported_files['synthetic_data'] = synth_filename\n",
    "        \n",
    "        # Export temporal aggregation data\n",
    "        if 'temporal_validation' in self.validation_results:\n",
    "            temporal_data = self.validation_results['temporal_validation']\n",
    "            \n",
    "            if 'original_aggregated' in temporal_data:\n",
    "                temporal_orig_filename = f\"temporal_original_{timestamp}.csv\"\n",
    "                temporal_data['original_aggregated'].to_csv(temporal_orig_filename)\n",
    "                exported_files['temporal_original'] = temporal_orig_filename\n",
    "            \n",
    "            if 'synthetic_aggregated' in temporal_data:\n",
    "                temporal_synth_filename = f\"temporal_synthetic_{timestamp}.csv\"\n",
    "                temporal_data['synthetic_aggregated'].to_csv(temporal_synth_filename)\n",
    "                exported_files['temporal_synthetic'] = temporal_synth_filename\n",
    "        \n",
    "        # Export network flow data\n",
    "        if 'network_validation' in self.validation_results:\n",
    "            network_data = self.validation_results['network_validation']\n",
    "            \n",
    "            for level_name, level_data in network_data.items():\n",
    "                # Export payer flows\n",
    "                if 'payer_centric' in level_data and 'original_flows' in level_data['payer_centric']:\n",
    "                    payer_flows = level_data['payer_centric']['original_flows']\n",
    "                    for payer, flow_data in payer_flows.items():\n",
    "                        safe_payer = payer.replace(' ', '_').replace(',', '').replace('.', '')\n",
    "                        payer_filename = f\"payer_flows_{level_name}_{safe_payer}_{timestamp}.csv\"\n",
    "                        flow_data.to_csv(payer_filename)\n",
    "                        exported_files[f'payer_{level_name}_{safe_payer}'] = payer_filename\n",
    "        \n",
    "        return exported_files\n",
    "    \n",
    "    def generate_comprehensive_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive validation report.\"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        report = {\n",
    "            'report_metadata': {\n",
    "                'generation_timestamp': datetime.now().isoformat(),\n",
    "                'original_data_size': len(self.original_data),\n",
    "                'synthetic_data_size': len(self.synthetic_data),\n",
    "                'scaling_factor': len(self.synthetic_data) / len(self.original_data)\n",
    "            },\n",
    "            'validation_configuration': {\n",
    "                'time_window_hours': validation_config.TIME_WINDOW_HOURS,\n",
    "                'payer_validation_count': validation_config.PAYER_VALIDATION_COUNT,\n",
    "                'payee_validation_count': validation_config.PAYEE_VALIDATION_COUNT,\n",
    "                'network_analysis_days': validation_config.NETWORK_ANALYSIS_DAYS,\n",
    "                'similarity_threshold': validation_config.SIMILARITY_THRESHOLD,\n",
    "                'validation_levels': {\n",
    "                    'company_level': validation_config.VALIDATE_COMPANY_LEVEL,\n",
    "                    'gics_level': validation_config.VALIDATE_GICS_LEVEL,\n",
    "                    'industry_level': validation_config.VALIDATE_INDUSTRY_LEVEL\n",
    "                }\n",
    "            },\n",
    "            'validation_results': self.validation_results,\n",
    "            'summary_assessment': self._generate_summary_assessment()\n",
    "        }\n",
    "        \n",
    "        # Save comprehensive report\n",
    "        report_filename = f\"comprehensive_validation_report_{timestamp}.json\"\n",
    "        with open(report_filename, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        return report_filename\n",
    "    \n",
    "    def _generate_summary_assessment(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate summary assessment of validation results.\"\"\"\n",
    "        overall_scores = self.validation_results.get('overall_scores', {})\n",
    "        \n",
    "        assessment = {\n",
    "            'overall_quality_score': overall_scores.get('overall_composite', 0.0),\n",
    "            'dimensional_scores': {\n",
    "                'temporal': overall_scores.get('temporal_average', 0.0),\n",
    "                'network': overall_scores.get('network_average', 0.0),\n",
    "                'entity_distribution': overall_scores.get('entity_distribution_average', 0.0)\n",
    "            },\n",
    "            'pass_fail_status': {\n",
    "                'overall_pass': overall_scores.get('overall_composite', 0.0) >= validation_config.SIMILARITY_THRESHOLD,\n",
    "                'temporal_pass': overall_scores.get('temporal_average', 0.0) >= validation_config.SIMILARITY_THRESHOLD,\n",
    "                'network_pass': overall_scores.get('network_average', 0.0) >= validation_config.SIMILARITY_THRESHOLD,\n",
    "                'entity_distribution_pass': overall_scores.get('entity_distribution_average', 0.0) >= validation_config.SIMILARITY_THRESHOLD\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        if not assessment['pass_fail_status']['temporal_pass']:\n",
    "            recommendations.append(\"Consider adjusting VAE training parameters to better capture temporal patterns\")\n",
    "        \n",
    "        if not assessment['pass_fail_status']['network_pass']:\n",
    "            recommendations.append(\"Review network structure preservation - consider increasing latent dimension\")\n",
    "        \n",
    "        if not assessment['pass_fail_status']['entity_distribution_pass']:\n",
    "            recommendations.append(\"Examine entity-specific distributions - may need category-aware training\")\n",
    "        \n",
    "        if assessment['pass_fail_status']['overall_pass']:\n",
    "            recommendations.append(\"Synthetic data meets quality thresholds - ready for deployment\")\n",
    "        \n",
    "        assessment['recommendations'] = recommendations\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def create_executive_summary(self) -> str:\n",
    "        \"\"\"Create executive summary for stakeholders.\"\"\"\n",
    "        summary_assessment = self._generate_summary_assessment()\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "VAE SYNTHETIC DATA GENERATION - EXECUTIVE SUMMARY\n",
    "=====================================================\n",
    "\n",
    "Data Scale Achievement:\n",
    "• Original Dataset: {len(self.original_data):,} transactions\n",
    "• Synthetic Dataset: {len(self.synthetic_data):,} transactions\n",
    "• Scaling Factor: {len(self.synthetic_data) / len(self.original_data):.1f}x\n",
    "\n",
    "Quality Assessment:\n",
    "• Overall Quality Score: {summary_assessment['overall_quality_score']:.3f}/1.0\n",
    "• Temporal Pattern Preservation: {summary_assessment['dimensional_scores']['temporal']:.3f}/1.0\n",
    "• Network Relationship Preservation: {summary_assessment['dimensional_scores']['network']:.3f}/1.0\n",
    "• Entity Distribution Preservation: {summary_assessment['dimensional_scores']['entity_distribution']:.3f}/1.0\n",
    "\n",
    "Validation Status:\n",
    "• Overall Assessment: {'✓ PASS' if summary_assessment['pass_fail_status']['overall_pass'] else '✗ FAIL'}\n",
    "• Quality Threshold Met: {'Yes' if summary_assessment['pass_fail_status']['overall_pass'] else 'No'} (≥{validation_config.SIMILARITY_THRESHOLD:.2f})\n",
    "\n",
    "Key Findings:\n",
    "• Multi-dimensional validation across temporal, network, and entity levels\n",
    "• Statistical similarity preservation at transaction and aggregate levels\n",
    "• Privacy compliance with configurable anonymity requirements\n",
    "• Relationship pattern preservation across company, industry, and GICS levels\n",
    "\n",
    "Recommendations:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, rec in enumerate(summary_assessment['recommendations'], 1):\n",
    "            summary += f\"• {rec}\\n\"\n",
    "        \n",
    "        summary += \"\\n\" + \"=\" * 60\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Generate comprehensive validation report\n",
    "reporter = ValidationReporter(validation_results, original_data, synthetic_data)\n",
    "\n",
    "print(\"\\nExporting validation data and generating reports...\")\n",
    "\n",
    "# Export data files\n",
    "exported_files = reporter.export_validation_data()\n",
    "print(f\"\\nExported {len(exported_files)} data files:\")\n",
    "for file_type, filename in exported_files.items():\n",
    "    print(f\"  {file_type}: {filename}\")\n",
    "\n",
    "# Generate comprehensive report\n",
    "report_filename = reporter.generate_comprehensive_report()\n",
    "print(f\"\\nComprehensive report saved: {report_filename}\")\n",
    "\n",
    "# Display executive summary\n",
    "executive_summary = reporter.create_executive_summary()\n",
    "print(executive_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"VAE SYNTHETIC DATA GENERATION WITH MULTI-DIMENSIONAL VALIDATION COMPLETE\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"📊 Generated {len(synthetic_data):,} synthetic rows from {len(original_data):,} original rows\")\n",
    "print(f\"🔍 Validated across temporal, network, and entity distribution dimensions\")\n",
    "print(f\"🎯 Overall quality score: {validation_results.get('overall_scores', {}).get('overall_composite', 0):.3f}/1.0\")\n",
    "print(f\"✅ Ready for production deployment and scaling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}