# CELL 5: Create VAE Model

import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam
import numpy as np

print("ðŸ”„ Creating VAE model with improved diversity parameters...")
print(f"New LATENT_DIM: {config.LATENT_DIM}")
print(f"New LEARNING_RATE: {config.LEARNING_RATE}")
print(f"New BETA_KL: {config.BETA_KL}")

# Calculate input dimension from processed_data shape
input_dim = processed_data.shape[1]
print(f"Input dimension from processed_data: {input_dim}")

class DatabricksVAE:
    """Variational Autoencoder optimized for Azure Databricks financial data."""
    
    def __init__(self, input_dim, latent_dim=32, encoder_layers=[256, 128, 64], 
                 decoder_layers=[64, 128, 256], learning_rate=5e-4, beta_kl=0.5):
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.encoder_layers = encoder_layers
        self.decoder_layers = decoder_layers
        self.learning_rate = learning_rate
        self.beta_kl = beta_kl
        
        # Build the model
        self.encoder = self._build_encoder()
        self.decoder = self._build_decoder()
        self.vae = self._build_vae()
        
        # Compile with improved optimizer
        optimizer = Adam(learning_rate=self.learning_rate, clipnorm=1.0)
        self.vae.compile(optimizer=optimizer)
        
        print(f"âœ… VAE created: {input_dim}â†’{latent_dim} latent dimensions")
        print(f"   Encoder: {encoder_layers}")
        print(f"   Decoder: {decoder_layers}")
    
    def _build_encoder(self):
        inputs = layers.Input(shape=(self.input_dim,))
        x = inputs
        
        # Encoder layers with dropout for regularization
        for units in self.encoder_layers:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.BatchNormalization()(x)
            x = layers.Dropout(0.2)(x)
        
        # Latent space parameters
        z_mean = layers.Dense(self.latent_dim, name='z_mean')(x)
        z_log_var = layers.Dense(self.latent_dim, name='z_log_var')(x)
        
        return Model(inputs, [z_mean, z_log_var], name='encoder')
    
    def _build_decoder(self):
        latent_inputs = layers.Input(shape=(self.latent_dim,))
        x = latent_inputs
        
        # Decoder layers
        for units in self.decoder_layers:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.BatchNormalization()(x)
            x = layers.Dropout(0.1)(x)
        
        outputs = layers.Dense(self.input_dim, activation='sigmoid')(x)
        
        return Model(latent_inputs, outputs, name='decoder')
    
    def _sampling(self, args):
        z_mean, z_log_var = args
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon
    
    def _build_vae(self):
        inputs = layers.Input(shape=(self.input_dim,))
        z_mean, z_log_var = self.encoder(inputs)
        z = layers.Lambda(self._sampling, output_shape=(self.latent_dim,))([z_mean, z_log_var])
        outputs = self.decoder(z)
        
        vae = Model(inputs, outputs, name='vae')
        
        # VAE loss with improved Î²-VAE formulation
        reconstruction_loss = tf.reduce_mean(
            tf.keras.losses.binary_crossentropy(inputs, outputs)
        ) * self.input_dim
        
        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
        kl_loss = tf.reduce_mean(kl_loss) * -0.5
        
        total_loss = reconstruction_loss + self.beta_kl * kl_loss
        vae.add_loss(total_loss)
        
        return vae
    
    def train(self, data, epochs=100, batch_size=256, verbose=1):
        """Train the VAE model with simplified interface."""
        return self.vae.fit(
            data, data,
            epochs=epochs,
            batch_size=batch_size,
            verbose=verbose,
            shuffle=True
        )
    
    def generate(self, num_samples):
        """Generate synthetic samples."""
        random_latent = tf.random.normal(shape=(num_samples, self.latent_dim))
        return self.decoder(random_latent).numpy()
    
    def encode(self, data):
        """Encode data to latent space."""
        z_mean, z_log_var = self.encoder(data)
        return z_mean.numpy()

# Create model instance with new parameters
vae_model = DatabricksVAE(
    input_dim=input_dim,
    latent_dim=config.LATENT_DIM,
    encoder_layers=config.ENCODER_LAYERS,
    decoder_layers=config.DECODER_LAYERS,
    learning_rate=config.LEARNING_RATE,
    beta_kl=config.BETA_KL
)

print("âœ… VAE model recreated with improved parameters for diversity")
