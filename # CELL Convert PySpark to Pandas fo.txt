# CELL 7: Synthetic Data Generation - Diagnostic and Fix

import numpy as np
import pandas as pd

print("üîç DIAGNOSTIC: Checking synthetic data generation...")

# Generate synthetic data
num_synthetic = len(original_data)
print(f"Generating {num_synthetic:,} synthetic samples...")

# Step 1: Generate latent samples and decode
synthetic_processed = vae_model.generate(num_synthetic)
print(f"‚úÖ Generated synthetic features: {synthetic_processed.shape}")
print(f"   Feature range: [{synthetic_processed.min():.3f}, {synthetic_processed.max():.3f}]")

# Step 2: Check processor state
print(f"\nüîç PROCESSOR DIAGNOSTIC:")
print(f"   Fitted: {processor.fitted}")
print(f"   Feature dim: {processor.feature_dim}")
print(f"   Categorical columns: {len(processor.config.CATEGORICAL_COLUMNS)}")
print(f"   Label encoders: {len(processor.label_encoders)}")

# Check each categorical column encoder
for col in processor.config.CATEGORICAL_COLUMNS:
    if col in processor.label_encoders:
        encoder = processor.label_encoders[col]
        print(f"   {col}: {len(encoder.classes_)} classes")
    else:
        print(f"   {col}: NO ENCODER FOUND")

# Step 3: Manual reconstruction to diagnose issue
print(f"\nüîß MANUAL RECONSTRUCTION:")

# Initialize result dataframe
synthetic_data = pd.DataFrame()
feature_idx = 0

# Reconstruct categorical columns manually
print("Reconstructing categorical columns...")
for col in processor.config.CATEGORICAL_COLUMNS:
    if col in processor.label_encoders:
        encoder = processor.label_encoders[col]
        n_classes = len(encoder.classes_)
        
        print(f"  Processing {col} ({n_classes} classes)...")
        
        # Extract one-hot features for this column
        one_hot_features = synthetic_processed[:, feature_idx:feature_idx + n_classes]
        print(f"    One-hot shape: {one_hot_features.shape}")
        print(f"    One-hot range: [{one_hot_features.min():.3f}, {one_hot_features.max():.3f}]")
        
        # Convert to categorical indices
        categorical_indices = np.argmax(one_hot_features, axis=1)
        print(f"    Unique indices: {len(np.unique(categorical_indices))}")
        print(f"    Index range: [{categorical_indices.min()}, {categorical_indices.max()}]")
        
        # Ensure indices are within valid range
        categorical_indices = np.clip(categorical_indices, 0, n_classes - 1)
        
        # Decode to original values
        try:
            decoded_values = encoder.inverse_transform(categorical_indices)
            synthetic_data[col] = decoded_values
            
            unique_decoded = len(np.unique(decoded_values))
            print(f"    ‚úÖ Decoded to {unique_decoded} unique values")
            
            # Show sample values
            sample_values = np.unique(decoded_values)[:5]
            print(f"    Sample values: {sample_values}")
            
        except Exception as e:
            print(f"    ‚ùå Decoding failed: {str(e)}")
            # Fallback: use original data sample
            synthetic_data[col] = np.random.choice(original_data[col].values, size=num_synthetic)
        
        feature_idx += n_classes
    else:
        print(f"  ‚ùå No encoder for {col}")
        # Fallback: use original data sample
        synthetic_data[col] = np.random.choice(original_data[col].values, size=num_synthetic)

# Reconstruct numerical columns
print("Reconstructing numerical columns...")
numerical_features = synthetic_processed[:, feature_idx:]
print(f"  Numerical features shape: {numerical_features.shape}")

for i, col in enumerate(processor.config.NUMERICAL_COLUMNS):
    if i < numerical_features.shape[1]:
        if col in processor.scalers:
            scaler = processor.scalers[col]
            # Reshape for scaler
            feature_col = numerical_features[:, i].reshape(-1, 1)
            decoded_values = scaler.inverse_transform(feature_col).flatten()
            synthetic_data[col] = decoded_values
            
            print(f"  ‚úÖ {col}: range [{decoded_values.min():.2f}, {decoded_values.max():.2f}]")
        else:
            print(f"  ‚ùå No scaler for {col}")
            synthetic_data[col] = original_data[col].sample(n=num_synthetic, replace=True).values

# Apply business constraints
print("\nüîß Applying business constraints...")
synthetic_data['ed_amount'] = np.clip(synthetic_data['ed_amount'], 0.01, 1000000.0)
synthetic_data['fh_file_creation_date'] = synthetic_data['fh_file_creation_date'].astype(int)
synthetic_data['fh_file_creation_time'] = np.clip(synthetic_data['fh_file_creation_time'].astype(int), 0, 2359)

# Final validation
print(f"\n‚úÖ FINAL SYNTHETIC DATA:")
print(f"   Shape: {synthetic_data.shape}")
print(f"   Columns: {list(synthetic_data.columns)}")

# Check categorical diversity
print(f"\nüìä CATEGORICAL DIVERSITY CHECK:")
for col in ['payer_Company_Name', 'payer_industry', 'payer_GICS']:
    if col in synthetic_data.columns:
        unique_count = synthetic_data[col].nunique()
        orig_unique = original_data[col].nunique()
        print(f"   {col}: {unique_count} synthetic vs {orig_unique} original")

# Display sample
print(f"\nüìã Sample synthetic data:")
display(synthetic_data.head())

print(f"\nüéâ Synthetic data generation completed with manual reconstruction!")
